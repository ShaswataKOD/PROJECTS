{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-30T11:55:37.948242Z","iopub.execute_input":"2024-08-30T11:55:37.949225Z","iopub.status.idle":"2024-08-30T11:55:38.399616Z","shell.execute_reply.started":"2024-08-30T11:55:37.949153Z","shell.execute_reply":"2024-08-30T11:55:38.398572Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install required libraries\n!pip install transformers torch vaderSentiment\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T11:59:47.604632Z","iopub.execute_input":"2024-08-30T11:59:47.605084Z","iopub.status.idle":"2024-08-30T12:00:04.721842Z","shell.execute_reply.started":"2024-08-30T11:59:47.605040Z","shell.execute_reply":"2024-08-30T12:00:04.720682Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nCollecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Initialize the VADER sentiment analyzer\nanalyzer = SentimentIntensityAnalyzer()\n\n# Initialize DialoGPT\nmodel_name = \"microsoft/DialoGPT-medium\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Initialize chat history\nchat_history_ids = torch.tensor([]).to(torch.int64)  # Empty tensor for initial chat history\n\ndef analyze_sentiment(text):\n    sentiment_score = analyzer.polarity_scores(text)\n    return sentiment_score\n\ndef chat_with_bot(input_text, sentiment):\n    global chat_history_ids\n\n    # Customize response based on sentiment\n    if sentiment['compound'] >= 0.05:\n        response_prefix = \"That's great to hear! \"\n    elif sentiment['compound'] <= -0.05:\n        response_prefix = \"I'm sorry to hear that. \"\n    else:\n        response_prefix = \"Thanks for sharing. \"\n\n    # Tokenize input text and append end-of-sequence token\n    new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n    \n    # Create attention mask\n    attention_mask = torch.ones(new_user_input_ids.shape, dtype=torch.long)\n    \n    # Concatenate new input with previous chat history\n    if chat_history_ids.size(0) > 0:\n        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n        attention_mask = torch.cat([torch.ones(chat_history_ids.shape, dtype=torch.long), attention_mask], dim=-1)\n    else:\n        bot_input_ids = new_user_input_ids\n    \n    # Generate response\n    chat_history_ids = model.generate(\n        bot_input_ids,\n        attention_mask=attention_mask,\n        max_length=1000,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    # Extract and decode response\n    bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n    \n    # Update chat history\n    chat_history_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]\n    \n    # Prepend sentiment-based response prefix\n    bot_output = response_prefix + bot_output\n    \n    return bot_output\n\n# Example usage\nuser_input = \"I'm feeling really stressed today.\"\nsentiment = analyze_sentiment(user_input)\nresponse = chat_with_bot(user_input, sentiment)\n\nprint(\"Sentiment Analysis:\", sentiment)\nprint(\"Chatbot Response:\", response)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T12:11:04.753222Z","iopub.execute_input":"2024-08-30T12:11:04.754225Z","iopub.status.idle":"2024-08-30T12:11:08.760363Z","shell.execute_reply.started":"2024-08-30T12:11:04.754177Z","shell.execute_reply":"2024-08-30T12:11:08.759221Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Chatbot Response: I'm sorry to hear that. I'm feeling really stressed today. I'm sorry to hear that. I'm feeling really stressed today.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}